[2024-09-12T06:11:50.313+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: shopeefood_crawl_and_report.load_db scheduled__2024-09-11T00:00:00+00:00 [queued]>
[2024-09-12T06:11:50.328+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: shopeefood_crawl_and_report.load_db scheduled__2024-09-11T00:00:00+00:00 [queued]>
[2024-09-12T06:11:50.329+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 2
[2024-09-12T06:11:50.353+0000] {taskinstance.py:2217} INFO - Executing <Task(PythonOperator): load_db> on 2024-09-11 00:00:00+00:00
[2024-09-12T06:11:50.361+0000] {standard_task_runner.py:60} INFO - Started process 670 to run task
[2024-09-12T06:11:50.364+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'shopeefood_crawl_and_report', 'load_db', 'scheduled__2024-09-11T00:00:00+00:00', '--job-id', '37', '--raw', '--subdir', 'DAGS_FOLDER/main_dag.py', '--cfg-path', '/tmp/tmp3pkmvifq']
[2024-09-12T06:11:50.366+0000] {standard_task_runner.py:88} INFO - Job 37: Subtask load_db
[2024-09-12T06:11:50.418+0000] {task_command.py:423} INFO - Running <TaskInstance: shopeefood_crawl_and_report.load_db scheduled__2024-09-11T00:00:00+00:00 [running]> on host f432431e979d
[2024-09-12T06:11:50.492+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='shopeefood_crawl_and_report' AIRFLOW_CTX_TASK_ID='load_db' AIRFLOW_CTX_EXECUTION_DATE='2024-09-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-11T00:00:00+00:00'
[2024-09-12T06:11:50.502+0000] {base.py:83} INFO - Using connection ID 'my_mysql_conn' for task execution.
[2024-09-12T06:11:50.788+0000] {python.py:202} INFO - Done. Returned value was: None
[2024-09-12T06:11:50.796+0000] {taskinstance.py:1149} INFO - Marking task as SUCCESS. dag_id=shopeefood_crawl_and_report, task_id=load_db, execution_date=20240911T000000, start_date=20240912T061150, end_date=20240912T061150
[2024-09-12T06:11:50.859+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-09-12T06:11:50.881+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-12T08:12:14.284+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: shopeefood_crawl_and_report.load_db scheduled__2024-09-11T00:00:00+00:00 [queued]>
[2024-09-12T08:12:14.294+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: shopeefood_crawl_and_report.load_db scheduled__2024-09-11T00:00:00+00:00 [queued]>
[2024-09-12T08:12:14.295+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 2
[2024-09-12T08:12:14.309+0000] {taskinstance.py:2217} INFO - Executing <Task(PythonOperator): load_db> on 2024-09-11 00:00:00+00:00
[2024-09-12T08:12:14.315+0000] {standard_task_runner.py:60} INFO - Started process 978 to run task
[2024-09-12T08:12:14.318+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'shopeefood_crawl_and_report', 'load_db', 'scheduled__2024-09-11T00:00:00+00:00', '--job-id', '10', '--raw', '--subdir', 'DAGS_FOLDER/main_dag.py', '--cfg-path', '/tmp/tmpaj3nql5t']
[2024-09-12T08:12:14.321+0000] {standard_task_runner.py:88} INFO - Job 10: Subtask load_db
[2024-09-12T08:12:14.368+0000] {task_command.py:423} INFO - Running <TaskInstance: shopeefood_crawl_and_report.load_db scheduled__2024-09-11T00:00:00+00:00 [running]> on host b944a8bf813f
[2024-09-12T08:12:14.441+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='shopeefood_crawl_and_report' AIRFLOW_CTX_TASK_ID='load_db' AIRFLOW_CTX_EXECUTION_DATE='2024-09-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-11T00:00:00+00:00'
[2024-09-12T08:12:14.618+0000] {base.py:83} INFO - Using connection ID 'my_mysql_conn' for task execution.
[2024-09-12T08:12:14.658+0000] {taskinstance.py:2731} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/pymysql/connections.py", line 649, in connect
    sock = socket.create_connection(
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/pymysql/connections.py", line 361, in __init__
    self.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/pymysql/connections.py", line 716, in connect
    raise exc
pymysql.err.OperationalError: (2003, "Can't connect to MySQL server on '172.19.0.5' ([Errno 111] Connection refused)")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 444, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 414, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 200, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 217, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/load/load_to_mysql.py", line 18, in load_data
    data_final_df.to_sql(name=table_name, con=engine, if_exists='replace', index=False)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/core/generic.py", line 2878, in to_sql
    return sql.to_sql(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 768, in to_sql
    with pandasSQL_builder(con, schema=schema, need_transaction=True) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/pymysql/connections.py", line 361, in __init__
    self.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/pymysql/connections.py", line 716, in connect
    raise exc
sqlalchemy.exc.OperationalError: (pymysql.err.OperationalError) (2003, "Can't connect to MySQL server on '172.19.0.5' ([Errno 111] Connection refused)")
(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2024-09-12T08:12:14.686+0000] {taskinstance.py:1149} INFO - Marking task as UP_FOR_RETRY. dag_id=shopeefood_crawl_and_report, task_id=load_db, execution_date=20240911T000000, start_date=20240912T081214, end_date=20240912T081214
[2024-09-12T08:12:14.700+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 10 for task load_db ((pymysql.err.OperationalError) (2003, "Can't connect to MySQL server on '172.19.0.5' ([Errno 111] Connection refused)")
(Background on this error at: https://sqlalche.me/e/14/e3q8); 978)
[2024-09-12T08:12:14.733+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-09-12T08:12:14.750+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-12T08:39:10.159+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: shopeefood_crawl_and_report.load_db scheduled__2024-09-11T00:00:00+00:00 [queued]>
[2024-09-12T08:39:10.168+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: shopeefood_crawl_and_report.load_db scheduled__2024-09-11T00:00:00+00:00 [queued]>
[2024-09-12T08:39:10.169+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 2
[2024-09-12T08:39:10.181+0000] {taskinstance.py:2217} INFO - Executing <Task(PythonOperator): load_db> on 2024-09-11 00:00:00+00:00
[2024-09-12T08:39:10.186+0000] {standard_task_runner.py:60} INFO - Started process 437 to run task
[2024-09-12T08:39:10.189+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'shopeefood_crawl_and_report', 'load_db', 'scheduled__2024-09-11T00:00:00+00:00', '--job-id', '9', '--raw', '--subdir', 'DAGS_FOLDER/main_dag.py', '--cfg-path', '/tmp/tmpq5p0tgcx']
[2024-09-12T08:39:10.191+0000] {standard_task_runner.py:88} INFO - Job 9: Subtask load_db
[2024-09-12T08:39:10.238+0000] {task_command.py:423} INFO - Running <TaskInstance: shopeefood_crawl_and_report.load_db scheduled__2024-09-11T00:00:00+00:00 [running]> on host 329a68cb1c0e
[2024-09-12T08:39:10.302+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='shopeefood_crawl_and_report' AIRFLOW_CTX_TASK_ID='load_db' AIRFLOW_CTX_EXECUTION_DATE='2024-09-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-11T00:00:00+00:00'
[2024-09-12T08:39:10.420+0000] {taskinstance.py:2731} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 444, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 414, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 200, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 217, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/load/load_to_mysql.py", line 8, in load_data
    connection = hook.get_connection('my_mysql_conn')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/hooks/base.py", line 82, in get_connection
    conn = Connection.get_connection_from_secrets(conn_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/connection.py", line 514, in get_connection_from_secrets
    raise AirflowNotFoundException(f"The conn_id `{conn_id}` isn't defined")
airflow.exceptions.AirflowNotFoundException: The conn_id `my_mysql_conn` isn't defined
[2024-09-12T08:39:10.435+0000] {taskinstance.py:1149} INFO - Marking task as UP_FOR_RETRY. dag_id=shopeefood_crawl_and_report, task_id=load_db, execution_date=20240911T000000, start_date=20240912T083910, end_date=20240912T083910
[2024-09-12T08:39:10.446+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 9 for task load_db (The conn_id `my_mysql_conn` isn't defined; 437)
[2024-09-12T08:39:10.484+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-09-12T08:39:10.504+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-12T08:57:48.356+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: shopeefood_crawl_and_report.load_db scheduled__2024-09-11T00:00:00+00:00 [queued]>
[2024-09-12T08:57:48.371+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: shopeefood_crawl_and_report.load_db scheduled__2024-09-11T00:00:00+00:00 [queued]>
[2024-09-12T08:57:48.372+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 2
[2024-09-12T08:57:48.392+0000] {taskinstance.py:2217} INFO - Executing <Task(PythonOperator): load_db> on 2024-09-11 00:00:00+00:00
[2024-09-12T08:57:48.402+0000] {standard_task_runner.py:60} INFO - Started process 467 to run task
[2024-09-12T08:57:48.406+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'shopeefood_crawl_and_report', 'load_db', 'scheduled__2024-09-11T00:00:00+00:00', '--job-id', '10', '--raw', '--subdir', 'DAGS_FOLDER/main_dag.py', '--cfg-path', '/tmp/tmpd2sb2kj9']
[2024-09-12T08:57:48.409+0000] {standard_task_runner.py:88} INFO - Job 10: Subtask load_db
[2024-09-12T08:57:48.481+0000] {task_command.py:423} INFO - Running <TaskInstance: shopeefood_crawl_and_report.load_db scheduled__2024-09-11T00:00:00+00:00 [running]> on host cde7247848aa
[2024-09-12T08:57:48.585+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='shopeefood_crawl_and_report' AIRFLOW_CTX_TASK_ID='load_db' AIRFLOW_CTX_EXECUTION_DATE='2024-09-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-11T00:00:00+00:00'
[2024-09-12T08:57:48.771+0000] {taskinstance.py:2731} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 444, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 414, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 200, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 217, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/load/load_to_mysql.py", line 8, in load_data
    connection = hook.get_connection('my_mysql_conn')
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/hooks/base.py", line 82, in get_connection
    conn = Connection.get_connection_from_secrets(conn_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/connection.py", line 514, in get_connection_from_secrets
    raise AirflowNotFoundException(f"The conn_id `{conn_id}` isn't defined")
airflow.exceptions.AirflowNotFoundException: The conn_id `my_mysql_conn` isn't defined
[2024-09-12T08:57:48.786+0000] {taskinstance.py:1149} INFO - Marking task as UP_FOR_RETRY. dag_id=shopeefood_crawl_and_report, task_id=load_db, execution_date=20240911T000000, start_date=20240912T085748, end_date=20240912T085748
[2024-09-12T08:57:48.803+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 10 for task load_db (The conn_id `my_mysql_conn` isn't defined; 467)
[2024-09-12T08:57:48.821+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-09-12T08:57:48.842+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-12T11:10:29.214+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: shopeefood_crawl_and_report.load_db scheduled__2024-09-11T00:00:00+00:00 [queued]>
[2024-09-12T11:10:29.224+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: shopeefood_crawl_and_report.load_db scheduled__2024-09-11T00:00:00+00:00 [queued]>
[2024-09-12T11:10:29.225+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 2
[2024-09-12T11:10:29.242+0000] {taskinstance.py:2217} INFO - Executing <Task(PythonOperator): load_db> on 2024-09-11 00:00:00+00:00
[2024-09-12T11:10:29.249+0000] {standard_task_runner.py:60} INFO - Started process 1082 to run task
[2024-09-12T11:10:29.252+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'shopeefood_crawl_and_report', 'load_db', 'scheduled__2024-09-11T00:00:00+00:00', '--job-id', '10', '--raw', '--subdir', 'DAGS_FOLDER/main_dag.py', '--cfg-path', '/tmp/tmp04u_avnc']
[2024-09-12T11:10:29.254+0000] {standard_task_runner.py:88} INFO - Job 10: Subtask load_db
[2024-09-12T11:10:29.304+0000] {task_command.py:423} INFO - Running <TaskInstance: shopeefood_crawl_and_report.load_db scheduled__2024-09-11T00:00:00+00:00 [running]> on host bc7cb9c82b76
[2024-09-12T11:10:29.374+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='shopeefood_crawl_and_report' AIRFLOW_CTX_TASK_ID='load_db' AIRFLOW_CTX_EXECUTION_DATE='2024-09-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-09-11T00:00:00+00:00'
[2024-09-12T11:10:29.513+0000] {base.py:83} INFO - Using connection ID 'my_mysql_conn' for task execution.
[2024-09-12T11:10:29.635+0000] {python.py:202} INFO - Done. Returned value was: None
[2024-09-12T11:10:29.644+0000] {taskinstance.py:1149} INFO - Marking task as SUCCESS. dag_id=shopeefood_crawl_and_report, task_id=load_db, execution_date=20240911T000000, start_date=20240912T111029, end_date=20240912T111029
[2024-09-12T11:10:29.668+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-09-12T11:10:29.683+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
